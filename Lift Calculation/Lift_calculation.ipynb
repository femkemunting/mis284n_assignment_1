{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/milanvaghani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/milanvaghani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing packages\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter \n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "import string\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import operator\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('test_set.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column in DataFrame of word tokens from comments\n",
    "comments['Tokens'] = comments['comments'].apply(nltk.tokenize.word_tokenize)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Stop Words & Update to include some punctuation and car(s)\n",
    "stop_words = nltk.corpus.stopwords.words('english')+ list(string.punctuation)\n",
    "\n",
    "def removeSW(word_tokens):\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "#Apply the removeSW function to the word tokens in the 'comments' DF\n",
    "comments['Tokens'] = comments['Tokens'].apply(removeSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.read_csv('car_models_and_brands.csv', header=None, names=['Brand','Model'])\n",
    "\n",
    "#Create a list of all Models\n",
    "models_list = models['Model'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to clean tokens & replace models with Brands\n",
    "def token_cleaner(row):\n",
    "    clean_tokens = []\n",
    "    list_row = list(row) #Set --> List\n",
    "    for word in list_row:\n",
    "        word = word.lower()\n",
    "        if word in models_list: #Change any models that appear to their corresponding Brands\n",
    "            word = models['Brand'][models['Model']==word].tolist()[0]\n",
    "            if word not in clean_tokens:\n",
    "                clean_tokens.append(word.lower())\n",
    "        else:\n",
    "            if word not in clean_tokens:\n",
    "                clean_tokens.append(word.lower())\n",
    "    clean_tokens = [x.lower() for x in clean_tokens]\n",
    "    return clean_tokens\n",
    "\n",
    "comments['Tokens'] = comments['Tokens'].apply(token_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to clean tokens & replace models with Brands\n",
    "def token_cleaner(row):\n",
    "    clean_tokens = []\n",
    "    list_row = list(row) #Set --> List\n",
    "    for word in list_row:\n",
    "        word = word.lower()\n",
    "        if word in models_list: #Change any models that appear to their corresponding Brands\n",
    "            word = models['Brand'][models['Model']==word].tolist()[0]\n",
    "            if word not in clean_tokens:\n",
    "                clean_tokens.append(word.lower())\n",
    "        else:\n",
    "            if word not in clean_tokens:\n",
    "                clean_tokens.append(word.lower())\n",
    "    clean_tokens = [x.lower() for x in clean_tokens]\n",
    "    return clean_tokens\n",
    "\n",
    "comments['Tokens'] = comments['Tokens'].apply(token_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_makes = dict()\n",
    "for i in words_dict:\n",
    "    if i in list(set(models['Brand'])):\n",
    "        top_makes[i] = words_dict[i]\n",
    "\n",
    "k = Counter(top_makes)\n",
    "high = k.most_common(10)\n",
    "\n",
    "top_brands = []\n",
    "for i in high:\n",
    "    top_brands.append(i[0])\n",
    "\n",
    "df_freq = pd.DataFrame(index=top_brands, columns=['Frequency'])\n",
    "\n",
    "for i in range(0,len(high)):\n",
    "    df_freq['Frequency'][i] = high[i][1]\n",
    "\n",
    "print('\\n \\033[1m' + 'Brands with Highest Frequency:' + '\\033[0m')\n",
    "display(df_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of the unique combinations of 2 brands that appeared in the top 10 most frequent list\n",
    "top_brand_combos = list(itertools.combinations(top_brands,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that will calculate Lift\n",
    "def liftCalc(word_1,word_2):\n",
    "    w1freq = sum(comments.apply(lambda x: word_1 in x['Tokens'], axis=1))\n",
    "    w2freq = sum(comments.apply(lambda x: word_2 in x['Tokens'], axis=1))\n",
    "    bothfreq = sum(comments.apply(lambda x: word_1 in x['Tokens'] and word_2 in x['Tokens'], axis=1))\n",
    "    return float(bothfreq/len(comments))/((float(w1freq)/len(comments))*(float(w2freq)/len(comments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize empty dictionary for lift values\n",
    "brand_lifts = dict()\n",
    "\n",
    "#iterate over brand combinations, calculate lift, save to dictionary\n",
    "for i in range(0,len(top_brand_combos)):\n",
    "    a,b = top_brand_combos[i]\n",
    "    brands = (a,b)\n",
    "    lift = liftCalc(a,b)\n",
    "    brand_lifts[brands] = lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lifts = pd.DataFrame(columns=top_brands,index=top_brands)\n",
    "for brand in top_brands:\n",
    "    df_lifts[brand][brand] = '-'\n",
    "for brands in brand_lifts:\n",
    "    a,b = brands\n",
    "    df_lifts[a][b] = (brand_lifts[brands])\n",
    "    df_lifts[b][a] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lift values for brand associations\n",
    "df_lifts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
