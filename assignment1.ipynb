{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a06735-2055-4bd7-b015-d4a44d9328f1",
   "metadata": {},
   "source": [
    "# Analytics For Unstructured Data: Group Assignment #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311e102-1f3d-43d0-a7c0-64321bff3b96",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bf3f6a-0478-4d8a-a7f1-39109653d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code if using Colab to run Selenium\n",
    "\n",
    "# Make sure to go to Runtime -> Change runtime and set GPU as hardware accelerator\n",
    "\n",
    "# !kill -9 -1 # Use this line to delete this VM and start a new one.\n",
    "# The above line deletes all files and folders from the current VM and allocates a new one.\n",
    "\n",
    "#Selenium is an open-source tool that automates web browsers.\n",
    "!pip install selenium\n",
    "!apt-get -q update   #Used to handle installation and removal of softwares and libraries\n",
    "!apt install -yq chromium-chromedriver #ChromeDriver is a separate executable that Selenium WebDriver uses to control Chrome.\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "import operator\n",
    "import io\n",
    "import csv\n",
    "import re\n",
    "import decimal\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from itertools import combinations\n",
    "import shutil\n",
    "from tempfile import NamedTemporaryFile\n",
    "#WebDriver is a browser automation framework that works with open source APIs.\n",
    "#The framework operates by accepting commands, sending those commands to a browser, and interacting with applications.\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "#headless means running chrome with chrome.exe\n",
    "chrome_options.add_argument('--headless')\n",
    "\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953b931-5967-47bb-90b7-2def4cf9e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Create DataFrame\n",
    "comments = pd.DataFrame(columns = ['Date','user_id','comments'])\n",
    "\n",
    "\n",
    "#Srape Dates, Usernames, and Comments from most recent 115 pages (about 5000 comments)\n",
    "for i in range(320,436):\n",
    "\n",
    "    #address where scraping\n",
    "    webpage = 'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p' + str(i)\n",
    "    driver.get(webpage)\n",
    "\n",
    "    ids = driver.find_elements(By.XPATH,\"//*[contains(@id,'Comment_')]\")\n",
    "\n",
    "    comment_ids = []\n",
    "\n",
    "    for i in ids:\n",
    "        comment_ids.append(i.get_attribute('id'))\n",
    "\n",
    "    #check if there is a blockquote (used in replies to comments) and remove\n",
    "    for x in comment_ids:\n",
    "        try:\n",
    "            element = driver.find_elements(By.XPATH,'//*[@id=\"' + x +'\"]/div/div[3]/div/div[1]/blockquote')[0]\n",
    "            driver.execute_script(\"\"\"\n",
    "                var element = arguments[0];\n",
    "                element.parentNode.removeChild(element);\n",
    "                \"\"\", element)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for x in comment_ids:\n",
    "\n",
    "        #Extract dates from for each user on a page\n",
    "        user_date = driver.find_elements(By.XPATH,'//*[@id=\"' + x +'\"]/div/div[2]/div[2]/span[1]/a/time')[0]\n",
    "        date = user_date.get_attribute('title')\n",
    "\n",
    "        #Extract user ids from each user on a page\n",
    "        userid_element = driver.find_elements(By.XPATH,'//*[@id=\"' + x +'\"]/div/div[2]/div[1]/span[1]/a[2]')[0]\n",
    "        userid = userid_element.text\n",
    "\n",
    "        #Extract Message for each user on a page\n",
    "        user_message = driver.find_elements(By.XPATH,'//*[@id=\"' + x +'\"]/div/div[3]/div/div[1]')[0]\n",
    "\n",
    "        comment = user_message.text\n",
    "\n",
    "\n",
    "        #Adding date, userid and comment for each user in a dataframe\n",
    "        comments.loc[len(comments)] = [date,userid,comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7914d25-68e4-41bb-8434-a9a7df9ee50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the location as per the file destination you want it to be\n",
    "comments.to_csv('Edmunds_scraped2.0.csv')\n",
    "#If working from CSV instead of scraping originally, read in CSV\n",
    "comments = pd.read_csv('Edmunds_scraped2.0.csv',  index_col=0)\n",
    "\n",
    "#Nulls don't play well with the tokenizer, so drop nulls\n",
    "comments.dropna(inplace=True)\n",
    "comments.to_csv('Edmunds_scraped2.0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e49e85",
   "metadata": {},
   "source": [
    "## Task A: Find and Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d01309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "output_file = 'find_and_replace.csv'  # The file where the modified data will be stored\n",
    "input_file = 'Edmunds_scraped2.0.csv'     # The file containing the original data\n",
    "replacement_file = 'car_models_and_brands.csv'  # The file containing original and replacement words\n",
    "\n",
    "# Create a temporary file to write the changes before moving it to the final location\n",
    "tempfile = NamedTemporaryFile(mode='w', delete=False, newline='', encoding='utf-8')\n",
    "\n",
    "def load_replacements(replacement_file):\n",
    "    \"\"\"\n",
    "    Load word replacements from a CSV file into a dictionary.\n",
    "    The right column contains words to be replaced by the corresponding words in the left column.\n",
    "    \"\"\"\n",
    "    replacements = {}\n",
    "    with open(replacement_file, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            original, replacement = row[1].lower(), row[0].lower()  # Ensure lowercase comparison\n",
    "            replacements[original] = replacement\n",
    "    return replacements\n",
    "\n",
    "def replace_words_in_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Replace words in the input text according to the replacements dictionary.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split text into words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # Remove punctuation from the word\n",
    "        word_clean = re.sub(r'[^\\w\\s]', '', word).lower() \n",
    "        # Replace word if it's in the replacements dictionary\n",
    "        if word_clean in replacements:\n",
    "            new_word = replacements[word_clean]\n",
    "            # Preserve original word's punctuation and case\n",
    "            new_words.append(re.sub(word_clean, new_word, word, flags=re.IGNORECASE))\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def process_file(input_file, output_file, replacements):\n",
    "    \"\"\"\n",
    "    Read the input file, perform word replacements, and write the modified content to the output file.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, tempfile:\n",
    "        reader = csv.reader(infile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # Write the header if present\n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        for row in reader:\n",
    "            new_row = [replace_words_in_text(col, replacements) for col in row]\n",
    "            writer.writerow(new_row)\n",
    "\n",
    "    # Move the temp file to the final output location\n",
    "    shutil.move(tempfile.name, output_file)\n",
    "\n",
    "def main():\n",
    "    # Load the replacement words from the replacement CSV file\n",
    "    replacements = load_replacements(replacement_file)\n",
    "    \n",
    "    # Process the input file and apply the replacements\n",
    "    process_file(input_file, output_file, replacements)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd1eb8",
   "metadata": {},
   "source": [
    "## Task B: Lift Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d713ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables and data structures\n",
    "df_lift = pd.DataFrame(columns=['word1', 'word2', 'lift_value'])  # To store lift values\n",
    "word_frequency = {}  # Dictionary to store word frequency in posts\n",
    "word_pair_frequency = defaultdict(dict)  # Dictionary to store word pair co-occurrence frequency\n",
    "results_dict = {}  # Dictionary to store results with lift values for word pairs\n",
    "file_length = 0  # Number of rows in the input file\n",
    "itr = 0  # Row iterator for the lift DataFrame\n",
    "\n",
    "# File paths\n",
    "input_file = 'find_and_replace.csv'  # Input data file\n",
    "pair_keys_file = 'edmunds_pair_keys.txt'  # File containing the words to calculate lift\n",
    "output_lift_values = 'Lift_Values.csv'  # Output file for lift values\n",
    "output_lift_matrix = 'Lift_Matrix.csv'  # Output file for lift matrix\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and tokenize text (removes punctuation and stopwords)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans a given text by removing punctuation, converting it to lowercase,\n",
    "    and tokenizing it, ignoring any stopwords.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Step 1: Load the words from the edmunds_pair_keys.txt file and generate all pairs\n",
    "def load_word_pairs(filename):\n",
    "    \"\"\"\n",
    "    Loads words from a file where words are comma-separated in each row.\n",
    "    Returns a list of all possible word pairs for each row.\n",
    "    \"\"\"\n",
    "    word_pairs = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Generate all possible word pairs from each row\n",
    "            pairs = list(combinations(row, 2))\n",
    "            word_pairs.extend(pairs)\n",
    "\n",
    "    return word_pairs\n",
    "\n",
    "# Step 2: Process the input CSV file to extract posts and clean the text\n",
    "def process_input_file(input_filename):\n",
    "    \"\"\"\n",
    "    Processes the input CSV file to extract and clean posts. Each post is tokenized,\n",
    "    cleaned of punctuation and stopwords, and stored in a list.\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    global file_length\n",
    "    df = pd.read_csv(input_filename)  # Load the CSV file into a DataFrame\n",
    "\n",
    "    # Assuming 'comments' is the column that contains the text\n",
    "    for index, row in df.iterrows():\n",
    "        cleaned_post = clean_text(row['comments'])  # Clean and tokenize the post\n",
    "        posts.append(cleaned_post)\n",
    "\n",
    "    file_length = len(df)  # Get the total number of rows\n",
    "    return posts\n",
    "\n",
    "# Step 3: Calculate word frequencies and word pair co-occurrences (distance ≥ 5 words)\n",
    "def calculate_frequencies(posts):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of individual words and word pairs within the posts.\n",
    "    Updates the global word_frequency and word_pair_frequency dictionaries.\n",
    "    Only considers word pairs that are 5 or more words apart.\n",
    "    \"\"\"\n",
    "    global word_frequency, word_pair_frequency\n",
    "\n",
    "    for post in posts:\n",
    "        word_positions = {}  # Dictionary to track positions of each word\n",
    "\n",
    "        # Track word positions\n",
    "        for idx, word in enumerate(post):\n",
    "            if word not in word_positions:\n",
    "                word_positions[word] = []\n",
    "            word_positions[word].append(idx)\n",
    "\n",
    "        # Count word frequencies\n",
    "        unique_words = set(post)  # Track unique words in the post to avoid double counting\n",
    "        for word in unique_words:\n",
    "            word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "\n",
    "        # Count word pair co-occurrences with distance check\n",
    "        for word1 in word_positions:\n",
    "            for word2 in word_positions:\n",
    "                if word1 != word2:\n",
    "                    # Check if the words are 5 or more positions apart\n",
    "                    for pos1 in word_positions[word1]:\n",
    "                        for pos2 in word_positions[word2]:\n",
    "                            if abs(pos1 - pos2) >= 5:\n",
    "                                word_pair_frequency[word1][word2] = word_pair_frequency[word1].get(word2, 0) + 1\n",
    "\n",
    "# Step 4: Calculate the lift between word pairs\n",
    "def calculate_lift(word_pairs):\n",
    "    \"\"\"\n",
    "    Calculates the lift between word pairs using the formula:\n",
    "    Lift(word1, word2) = P(word1 AND word2) / (P(word1) * P(word2))\n",
    "    Lift is written to the lift values CSV and stored in a DataFrame for further processing.\n",
    "    \"\"\"\n",
    "    global itr\n",
    "\n",
    "    for word1, word2 in word_pairs:\n",
    "        # Get the frequency of word1, word2, and their co-occurrence\n",
    "        freq_word1 = word_frequency.get(word1, 0)\n",
    "        freq_word2 = word_frequency.get(word2, 0)\n",
    "        co_occurrence = word_pair_frequency.get(word1, {}).get(word2, 0)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        p_word1 = freq_word1 / file_length if freq_word1 else 0\n",
    "        p_word2 = freq_word2 / file_length if freq_word2 else 0\n",
    "        p_word1_and_word2 = co_occurrence / file_length if co_occurrence else 0\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if p_word1 > 0 and p_word2 > 0:\n",
    "            lift_value = p_word1_and_word2 / (p_word1 * p_word2) if (p_word1 * p_word2) > 0 else 0\n",
    "        else:\n",
    "            lift_value = 0\n",
    "\n",
    "        # Store lift value in DataFrame\n",
    "        df_lift.loc[itr] = [word1, word2, lift_value]\n",
    "        itr += 1\n",
    "\n",
    "# Step 5: Write lift values and matrix to CSV\n",
    "def save_results():\n",
    "    \"\"\"\n",
    "    Writes the calculated lift values to a CSV file and also generates a lift matrix,\n",
    "    saving it to another CSV.\n",
    "    \"\"\"\n",
    "    # Save lift values DataFrame to CSV\n",
    "    df_lift.to_csv(output_lift_values, index=False)\n",
    "\n",
    "    # Generate lift matrix\n",
    "    lift_matrix = pd.pivot_table(df_lift, values='lift_value', index='word1', columns='word2', fill_value=0)\n",
    "    lift_matrix.to_csv(output_lift_matrix)\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Load word pairs\n",
    "    word_pairs = load_word_pairs(pair_keys_file)\n",
    "\n",
    "    # Step 2: Process the input file to extract posts\n",
    "    posts = process_input_file(input_file)\n",
    "\n",
    "    # Step 3: Calculate frequencies\n",
    "    calculate_frequencies(posts)\n",
    "\n",
    "    # Step 4: Calculate lift values\n",
    "    calculate_lift(word_pairs)\n",
    "\n",
    "    # Step 5: Save results\n",
    "    save_results()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9d01c-9d73-422c-b5db-e4d8446db2df",
   "metadata": {},
   "source": [
    "## Task A: Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a46a0-1fd3-4fa6-9703-46cd6241d927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56c5e0d0-6160-4e9b-9c53-07c0ea277f0b",
   "metadata": {},
   "source": [
    "## Task B: Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e054e-47be-4cde-bf65-310dfaffd715",
   "metadata": {},
   "source": [
    "Replace frequently occuring car models with brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db476bd9-fa84-4f6d-ab10-9740728109d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58 duplicated car brands and models in the key.\n",
      "These are the remaining model duplicates. Multiple brands can have the same name for a model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>buick</td>\n",
       "      <td>century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>toyota</td>\n",
       "      <td>century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acura</td>\n",
       "      <td>legend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>honda</td>\n",
       "      <td>legend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>hyundai</td>\n",
       "      <td>matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>toyota</td>\n",
       "      <td>matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>mercedes benz</td>\n",
       "      <td>mercedes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>mercedes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>kia</td>\n",
       "      <td>optima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>kia,</td>\n",
       "      <td>optima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>kia.</td>\n",
       "      <td>optima</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>passat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>volkwagen</td>\n",
       "      <td>passat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand     Model\n",
       "44           buick   century\n",
       "422         toyota   century\n",
       "1            acura    legend\n",
       "171          honda    legend\n",
       "224        hyundai    matrix\n",
       "446         toyota    matrix\n",
       "313  mercedes benz  mercedes\n",
       "314  mercedes-benz  mercedes\n",
       "265            kia    optima\n",
       "266           kia,    optima\n",
       "267           kia.    optima\n",
       "510     volkswagen    passat\n",
       "522      volkwagen    passat"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need to decide what to do with these duplicates. The replacement value for each model key will just be whatever the last brand is for that model.\n"
     ]
    }
   ],
   "source": [
    "# informal method disregarding example script\n",
    "\n",
    "df = pd.read_csv('sample data.csv', names = ['user', 'date', 'message'])\n",
    "bm = pd.read_csv('car_models_and_brands.csv')\n",
    "\n",
    "# create dictionary with model as key and brand as value\n",
    "\n",
    "# make sure there aren't any repeating keys\n",
    "n_dupl = bm.duplicated(keep=False).sum()\n",
    "print(f'There are {n_dupl} duplicated car brands and models in the key.')\n",
    "# drop rows with duplicate brand and model\n",
    "bm = bm.drop_duplicates()\n",
    "\n",
    "# check remaining model duplicates\n",
    "print('These are the remaining model duplicates. Multiple brands can have the same name for a model.')\n",
    "display(bm[bm.duplicated(subset = 'Model', keep=False)].sort_values(by = 'Model'))\n",
    "print('We need to decide what to do with these duplicates. The replacement value for each model key will just be whatever the last brand is for that model.')\n",
    "\n",
    "# create dictionary\n",
    "bm_dict = dict(zip(bm.Model, bm.Brand))\n",
    "\n",
    "# replace models with brand names\n",
    "def replace_model(message, mapping_dict):\n",
    "    for model, brand in mapping_dict.items():\n",
    "        # Use regex to ensure whole word replacement\n",
    "        message = re.sub(r'\\b{}\\b'.format(re.escape(model)), brand, message)\n",
    "    return message\n",
    "df['message'] = df['message'].apply(lambda msg: replace_model(msg, bm_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3255d59-0da0-44b2-acc3-e13df13fc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat version. Filled in Barua's pseudo code\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "# Filepaths\n",
    "output_file = 'replacement_sample_data.csv'  # The file where the modified data will be stored\n",
    "input_file = 'sample data.csv'               # The file containing the original data\n",
    "replacement_file = 'car_models_and_brands.csv'   # The file containing original and replacement words\n",
    "\n",
    "# Create a temporary file to write the changes before moving it to the final location\n",
    "tempfile = NamedTemporaryFile(mode='w', delete=False, newline='', encoding='utf-8')\n",
    "\n",
    "def load_replacements(replacement_file):\n",
    "    \"\"\"\n",
    "    Load word replacements from a CSV file into a dictionary.\n",
    "    The right column contains words to be replaced by the corresponding words in the left column.\n",
    "    \"\"\"\n",
    "    replacements = {}\n",
    "    with open(replacement_file, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            if len(row) >= 2:  # Ensure there are enough columns\n",
    "                original, replacement = row[1].lower(), row[0].lower()  # Ensure lowercase comparison\n",
    "                replacements[original] = replacement\n",
    "    return replacements\n",
    "\n",
    "def replace_words_in_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Replace words in the input text according to the replacements dictionary.\n",
    "    \"\"\"\n",
    "    # Define a regex pattern that matches any of the words to be replaced\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(key) for key in replacements.keys()) + r')\\b', re.IGNORECASE)\n",
    "    \n",
    "    def replace(match):\n",
    "        word = match.group(0).lower()\n",
    "        return replacements.get(word, word)\n",
    "    \n",
    "    # Substitute words using the pattern and replacement function\n",
    "    return pattern.sub(replace, text)\n",
    "\n",
    "def process_file(input_file, output_file, replacements):\n",
    "    \"\"\"\n",
    "    Read the input file, perform word replacements, and write the modified content to the output file.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         NamedTemporaryFile(mode='w', delete=False, newline='', encoding='utf-8') as tempfile:\n",
    "        \n",
    "        reader = csv.reader(infile, delimiter=',', quotechar='\"')\n",
    "        writer = csv.writer(tempfile, delimiter=',', quotechar='\"')\n",
    "        \n",
    "        for row in reader:\n",
    "            # Assuming text is in the first column (index 0) for replacement\n",
    "            if row:\n",
    "                row[2] = replace_words_in_text(row[2], replacements)\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    # Move the tempfile to the final output file\n",
    "    shutil.move(tempfile.name, output_file)\n",
    "\n",
    "def main():\n",
    "    # Load the replacement words from the replacement CSV file\n",
    "    replacements = load_replacements(replacement_file)\n",
    "    \n",
    "    # Process the input file and apply the replacements\n",
    "    process_file(input_file, output_file, replacements)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7630093-4ec3-4817-bfda-f38a3ea2c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare files to make sure it worked right\n",
    "#pd.read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28f9fc-1bbd-4e4b-8a30-87790032c608",
   "metadata": {},
   "source": [
    "Brand frequency counts (increase counter for each message that contains word). \n",
    "\n",
    "**Should the unique brands that we count be the same as the ones in the 'car_models_and_brands.csv' file? Instructions said it wasn't exhaustive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9824a93-e9ba-4197-bc82-063ebc061e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>car</td>\n",
       "      <td>2992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bmw</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hyundai</td>\n",
       "      <td>1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hyundai.</td>\n",
       "      <td>1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kia</td>\n",
       "      <td>1712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>kia.</td>\n",
       "      <td>1712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>toyota</td>\n",
       "      <td>1681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audi</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acura</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sedan</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>honda</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pontiac</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>infiniti</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>seat</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mercedes</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ford</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>volvo</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dodge</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cadillac</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nissan.</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nissan</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chevrolet</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mazda</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>lincoln</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buick</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>subaru</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chrysler</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>saturn</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mitsubishi</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>suzuki</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mercury</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hyundai,</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>toyata</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mercedes benz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hyndai kia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>volkwagen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>kia,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            brand  frequency\n",
       "5             car       2992\n",
       "2             bmw       2044\n",
       "12        hyundai       1794\n",
       "14       hyundai.       1794\n",
       "16            kia       1712\n",
       "18           kia.       1712\n",
       "35         toyota       1681\n",
       "1            audi        845\n",
       "0           acura        805\n",
       "31          sedan        728\n",
       "10          honda        716\n",
       "28        pontiac        632\n",
       "15       infiniti        555\n",
       "30           seat        479\n",
       "21       mercedes        312\n",
       "9            ford        308\n",
       "38          volvo        293\n",
       "8           dodge        290\n",
       "4        cadillac        285\n",
       "27        nissan.        273\n",
       "26         nissan        273\n",
       "36     volkswagen        209\n",
       "6       chevrolet        206\n",
       "20          mazda        200\n",
       "19        lincoln        185\n",
       "3           buick        160\n",
       "32         subaru        117\n",
       "7        chrysler         64\n",
       "29         saturn         42\n",
       "25     mitsubishi         23\n",
       "33         suzuki         16\n",
       "24        mercury         12\n",
       "13       hyundai,          4\n",
       "23  mercedes-benz          0\n",
       "34         toyata          0\n",
       "22  mercedes benz          0\n",
       "11     hyndai kia          0\n",
       "37      volkwagen          0\n",
       "17           kia,          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Short solution not following example code\n",
    "# get unique car brand\n",
    "df = pd.read_csv('replacement_sample_data.csv', names = ['user', 'date', 'message'])\n",
    "bm = pd.read_csv('car_models_and_brands.csv')\n",
    "brands = bm['Brand'].unique()\n",
    "brands = brands[brands != 'problem']\n",
    "brands = sorted(brands) # sort brands\n",
    "freq = [df['message'].str.contains(brand).sum() for brand in brands]\n",
    "freq_df = pd.DataFrame({'brand': brands, 'frequency': freq})\n",
    "freq_df.sort_values('frequency', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195da353-c080-40c2-aa65-5244c3e4fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency counts\n",
    "\n",
    "\n",
    "# should I just be counting the car models or all word frequencies?\n",
    "\n",
    "# if I'm counting all word frequencies, then should I only count 1 for a word if it appears in a message (regardless of how many times it appears in that message)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1dd171e-2d73-4abf-b0b4-cdc8612b2364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kenne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies written to word_freq.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import io\n",
    "\n",
    "# Download stopwords from the NLTK package\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Input and output filenames\n",
    "input_filename = 'sample data.csv'  # Input file\n",
    "final_filename = 'final.csv'  # Intermediate file without the column header\n",
    "word_freq_output = 'word_freq.csv'  # Output file for word frequencies\n",
    "\n",
    "# extra stuff to reduce brand counts\n",
    "keys_filename = 'car_models_and_brands.csv'   # The file containing keys to restrict brand counts\n",
    "keys_file = pd.read_csv(keys_filename)\n",
    "keys_list = list(keys_file.iloc[:,0].values)\n",
    "keys_list = keys_list[keys_list != 'problem']\n",
    "\n",
    "# Function to clean and tokenize sentences\n",
    "def clean_and_tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Cleans a given sentence by removing punctuation and stopwords, converting text to lowercase,\n",
    "    and tokenizing the remaining words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and convert text to lowercase\n",
    "    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    return [word for word in sentence.split() if word not in stop_words]\n",
    "\n",
    "# Step 1: Remove header from the input CSV and create a new file without it\n",
    "def remove_header(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reads the input CSV file, removes the header, and writes the remaining rows\n",
    "    into a new output file.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        next(reader)  # Skip the header NOTE THAT SAMPLE DATA DOESN'T HAVE A HEADER\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Step 2: Extract and clean sentences from the text\n",
    "def extract_sentences(file):\n",
    "    \"\"\"\n",
    "    Extracts text data from the third column of the CSV file, splits it into sentences,\n",
    "    and cleans each sentence by removing punctuation and stopwords.\n",
    "    The result is a list with 3 levels: message, sentences, and words\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        sentences_clean = []\n",
    "        for row in reader:\n",
    "            # Assume text is in the third column (index 2)\n",
    "            text = row[2]\n",
    "            text_sentences_clean = []\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split based on punctuation\n",
    "            for sentence in sentences:\n",
    "                cleaned_tokens = clean_and_tokenize(sentence)\n",
    "                if cleaned_tokens:  # Avoid adding empty sentences\n",
    "                    text_sentences_clean.append(cleaned_tokens)\n",
    "            sentences_clean.append(text_sentences_clean)\n",
    "\n",
    "    return sentences_clean\n",
    "\n",
    "def replace_values_with_one(dictionary, keys_list):\n",
    "    \"\"\"\n",
    "    Replace values with 1 in the dictionary for keys present in the keys_list.\n",
    "    \n",
    "    Parameters:\n",
    "    dictionary (dict): The dictionary to be updated.\n",
    "    keys_list (list): The list of keys whose values need to be replaced with 1.\n",
    "    \n",
    "    Returns:\n",
    "    dict: The updated dictionary.\n",
    "    \"\"\"\n",
    "    return {k: 1 if k in keys_list else v for k, v in dictionary.items()}\n",
    "\n",
    "# Step 3: Calculate word frequencies\n",
    "def calculate_word_frequencies(messages, keys_list):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of each word in the given list of cleaned sentences.\n",
    "    \"\"\"\n",
    "    final_counter = defaultdict(int) # has default value of 0 when key doesn't exist\n",
    "    for message in messages:\n",
    "        word_counter = defaultdict(int) # restart for each message to ensure I only count brands once\n",
    "        for sentence in message:\n",
    "            for word in sentence:\n",
    "                word_counter[word] += 1\n",
    "        # remove multiple mentions of brand\n",
    "        word_counter = replace_values_with_one(word_counter, keys_list)\n",
    "        # update final dictionary\n",
    "        for key in word_counter:\n",
    "            final_counter[key] += word_counter[key]\n",
    "    return final_counter\n",
    "\n",
    "# Step 4: Write word frequencies to CSV\n",
    "def write_word_frequencies(word_freq, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word frequencies to the specified CSV file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['Word', 'Frequency'])\n",
    "        for word, freq in word_freq.items():\n",
    "            writer.writerow([word, freq])\n",
    "\n",
    "    print(f\"Word frequencies written to {output_file}\")\n",
    "\n",
    "# Main function to run all steps\n",
    "def main():\n",
    "    # Step 1: Remove header\n",
    "    remove_header(input_filename, final_filename)\n",
    "    \n",
    "    # Step 2: Extract and clean sentences\n",
    "    messages_clean = extract_sentences(final_filename)\n",
    "    \n",
    "    # Step 3: Calculate word frequencies\n",
    "    word_freq = calculate_word_frequencies(messages_clean, keys_list)\n",
    "    \n",
    "    # Step 4: Write word frequencies to CSV\n",
    "    write_word_frequencies(word_freq, word_freq_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "049aef24-ec5d-4c9e-8cc6-74eb851878e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kia</td>\n",
       "      <td>5982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hyundai</td>\n",
       "      <td>6179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bmws</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owners</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paid</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>badge</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rather</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>car</td>\n",
       "      <td>6134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>imho</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>im</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>glad</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>added</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dont</td>\n",
       "      <td>1577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>speak</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>assume</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>know</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nothing</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bmw</td>\n",
       "      <td>4224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>buyer</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>except</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>small</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>select</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fewyou</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>right</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>people</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pay</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>potential</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>understands</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>behind</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>worked</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hard</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>engineer</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>want</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ultimate</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>driving</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>experience</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>havent</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>driven</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>borrow</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>one</td>\n",
       "      <td>1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Frequency\n",
       "0           kia       5982\n",
       "1       hyundai       6179\n",
       "2          bmws        176\n",
       "3        owners        153\n",
       "4          paid         93\n",
       "5         badge         73\n",
       "6        rather        162\n",
       "7           car       6134\n",
       "8          imho        101\n",
       "9            im        998\n",
       "10         glad         49\n",
       "11        added         85\n",
       "12         dont       1577\n",
       "13        speak         37\n",
       "14       assume         89\n",
       "15         know        846\n",
       "16      nothing        245\n",
       "17          bmw       4224\n",
       "18        buyer         96\n",
       "19       except        122\n",
       "20        small        215\n",
       "21       select         26\n",
       "22       fewyou          1\n",
       "23        right        455\n",
       "24       people        848\n",
       "25          pay        261\n",
       "26    potential         41\n",
       "27  understands          4\n",
       "28       behind        118\n",
       "29       worked         35\n",
       "30         hard        266\n",
       "31     engineer         11\n",
       "32         want        822\n",
       "33     ultimate         43\n",
       "34      driving        726\n",
       "35   experience        274\n",
       "36       havent        122\n",
       "37       driven        280\n",
       "38       borrow          4\n",
       "39          one       1875"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('word_freq.csv')\n",
    "df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9c7e3-cb3f-43e3-9a77-c6c8d85f158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE!\n",
    "# could use nltk's sentence tokenizer and Count Vectorizer instead?\n",
    "\n",
    "# Create a tokenizer for splitting on punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\s*[\\.\\?!]\\s*')\n",
    "\n",
    "text = \"Hello world! This is a test. How are you doing today?\"\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = tokenizer.tokenize(text)\n",
    "\n",
    "print(sentences)\n",
    "\n",
    "# Count Vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
