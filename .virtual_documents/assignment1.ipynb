


import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns





from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
import time
import pandas as pd
import nltk
from collections import Counter 
import itertools
import numpy as np
import matplotlib.pyplot as plt
from sklearn import manifold
import string
import seaborn as sns
from nltk.tokenize import RegexpTokenizer
import string
import operator
import io


# you will have to watch the attached link youtube video which shows how to show install chrome driver on mac
# https://www.youtube.com/watch?v=m4-Z5KqDHpU
driver = webdriver.Chrome()


# Create DataFrame 
comments = pd.DataFrame(columns = ['Date','user_id','comments'])


#Srape Dates, Usernames, and Comments from most recent 100 pages (about 5000 comments)
for i in range(440,540):
    
    #address where scraping
    webpage = 'https://forums.edmunds.com/discussion/7526/general/x/midsize-sedans-2-0/p' + str(i)
    driver.get(webpage)

    ids = driver.find_elements(By.XPATH,"//*[contains(@id,'Comment_')]")

    comment_ids = []

    for i in ids:
        comment_ids.append(i.get_attribute('id'))

    #check if there is a blockquote (used in replies to comments) and remove
    for x in comment_ids: 
        try:
            element = driver.find_elements(By.XPATH,'//*[@id="' + x +'"]/div/div[3]/div/div[1]/blockquote')[0]
            driver.execute_script("""
                var element = arguments[0];
                element.parentNode.removeChild(element);
                """, element)
        except:
            pass    
    
    for x in comment_ids:
        
        #Extract dates from for each user on a page
        user_date = driver.find_elements(By.XPATH,'//*[@id="' + x +'"]/div/div[2]/div[2]/span[1]/a/time')[0]
        date = user_date.get_attribute('title')

        #Extract user ids from each user on a page
        userid_element = driver.find_elements(By.XPATH,'//*[@id="' + x +'"]/div/div[2]/div[1]/span[1]/a[2]')[0]
        userid = userid_element.text

        #Extract Message for each user on a page
        user_message = driver.find_elements(By.XPATH,'//*[@id="' + x +'"]/div/div[3]/div/div[1]')[0]
            
        comment = user_message.text

                                   
        #Adding date, userid and comment for each user in a dataframe    
        comments.loc[len(comments)] = [date,userid,comment]


#change the location as per the file destination you want it to be
comments.to_csv('/Users/milanvaghani/Desktop/Unstructed Machine Learning/Edmunds_scraped2.0.csv')














# informal method disregarding example script

df = pd.read_csv('sample data.csv', names = ['user', 'date', 'message'])
bm = pd.read_csv('car_models_and_brands.csv')

# create dictionary with model as key and brand as value

# make sure there aren't any repeating keys
n_dupl = bm.duplicated(keep=False).sum()
print(f'There are {n_dupl} duplicated car brands and models in the key.')
# drop rows with duplicate brand and model
bm = bm.drop_duplicates()

# check remaining model duplicates
print('These are the remaining model duplicates. Multiple brands can have the same name for a model.')
display(bm[bm.duplicated(subset = 'Model', keep=False)].sort_values(by = 'Model'))
print('We need to decide what to do with these duplicates. The replacement value for each model key will just be whatever the last brand is for that model.')

# create dictionary
bm_dict = dict(zip(bm.Model, bm.Brand))

# replace models with brand names
def replace_model(message, mapping_dict):
    for model, brand in mapping_dict.items():
        # Use regex to ensure whole word replacement
        message = re.sub(r'\b{}\b'.format(re.escape(model)), brand, message)
    return message
df['message'] = df['message'].apply(lambda msg: replace_model(msg, bm_dict))


# Chat version. Filled in Barua's pseudo code

import csv
import re
import shutil
from tempfile import NamedTemporaryFile

# Filepaths
output_file = 'replacement_sample_data.csv'  # The file where the modified data will be stored
input_file = 'sample data.csv'               # The file containing the original data
replacement_file = 'car_models_and_brands.csv'   # The file containing original and replacement words

# Create a temporary file to write the changes before moving it to the final location
tempfile = NamedTemporaryFile(mode='w', delete=False, newline='', encoding='utf-8')

def load_replacements(replacement_file):
    """
    Load word replacements from a CSV file into a dictionary.
    The right column contains words to be replaced by the corresponding words in the left column.
    """
    replacements = {}
    with open(replacement_file, 'r', encoding='utf-8') as csvfile:
        reader = csv.reader(csvfile, delimiter=',', quotechar='"')
        for row in reader:
            if len(row) >= 2:  # Ensure there are enough columns
                original, replacement = row[1].lower(), row[0].lower()  # Ensure lowercase comparison
                replacements[original] = replacement
    return replacements

def replace_words_in_text(text, replacements):
    """
    Replace words in the input text according to the replacements dictionary.
    """
    # Define a regex pattern that matches any of the words to be replaced
    pattern = re.compile(r'\b(?:' + '|'.join(re.escape(key) for key in replacements.keys()) + r')\b', re.IGNORECASE)
    
    def replace(match):
        word = match.group(0).lower()
        return replacements.get(word, word)
    
    # Substitute words using the pattern and replacement function
    return pattern.sub(replace, text)

def process_file(input_file, output_file, replacements):
    """
    Read the input file, perform word replacements, and write the modified content to the output file.
    """
    with open(input_file, 'r', encoding='utf-8') as infile, \
         NamedTemporaryFile(mode='w', delete=False, newline='', encoding='utf-8') as tempfile:
        
        reader = csv.reader(infile, delimiter=',', quotechar='"')
        writer = csv.writer(tempfile, delimiter=',', quotechar='"')
        
        for row in reader:
            # Assuming text is in the first column (index 0) for replacement
            if row:
                row[2] = replace_words_in_text(row[2], replacements)
            writer.writerow(row)
    
    # Move the tempfile to the final output file
    shutil.move(tempfile.name, output_file)

def main():
    # Load the replacement words from the replacement CSV file
    replacements = load_replacements(replacement_file)
    
    # Process the input file and apply the replacements
    process_file(input_file, output_file, replacements)

if __name__ == "__main__":
    main()


# compare files to make sure it worked right
#pd.read_csv()





# Short solution not following example code
# get unique car brand
df = pd.read_csv('replacement_sample_data.csv', names = ['user', 'date', 'message'])
bm = pd.read_csv('car_models_and_brands.csv')
brands = bm['Brand'].unique()
brands = brands[brands != 'problem']
brands = sorted(brands) # sort brands
freq = [df['message'].str.contains(brand).sum() for brand in brands]
freq_df = pd.DataFrame({'brand': brands, 'frequency': freq})
freq_df.sort_values('frequency', ascending = False)


# frequency counts


# should I just be counting the car models or all word frequencies?

# if I'm counting all word frequencies, then should I only count 1 for a word if it appears in a message (regardless of how many times it appears in that message)?



import csv
import re
import string
from collections import defaultdict
from nltk.corpus import stopwords
import nltk
import io

# Download stopwords from the NLTK package
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Input and output filenames
input_filename = 'sample data.csv'  # Input file
final_filename = 'final.csv'  # Intermediate file without the column header
word_freq_output = 'word_freq.csv'  # Output file for word frequencies

# extra stuff to reduce brand counts
keys_filename = 'car_models_and_brands.csv'   # The file containing keys to restrict brand counts
keys_file = pd.read_csv(keys_filename)
keys_list = list(keys_file.iloc[:,0].values)
keys_list = keys_list[keys_list != 'problem']

# Function to clean and tokenize sentences
def clean_and_tokenize(sentence):
    """
    Cleans a given sentence by removing punctuation and stopwords, converting text to lowercase,
    and tokenizing the remaining words.
    """
    # Remove punctuation and convert text to lowercase
    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())

    # Tokenize and remove stopwords
    return [word for word in sentence.split() if word not in stop_words]

# Step 1: Remove header from the input CSV and create a new file without it
def remove_header(input_file, output_file):
    """
    Reads the input CSV file, removes the header, and writes the remaining rows
    into a new output file.
    """
    with open(input_file, 'r') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:
        reader = csv.reader(infile)
        writer = csv.writer(outfile)
        next(reader)  # Skip the header NOTE THAT SAMPLE DATA DOESN'T HAVE A HEADER
        for row in reader:
            writer.writerow(row)

# Step 2: Extract and clean sentences from the text
def extract_sentences(file):
    """
    Extracts text data from the third column of the CSV file, splits it into sentences,
    and cleans each sentence by removing punctuation and stopwords.
    The result is a list with 3 levels: message, sentences, and words
    """
    

    with open(file, 'r', encoding='utf-8') as infile:
        reader = csv.reader(infile)
        sentences_clean = []
        for row in reader:
            # Assume text is in the third column (index 2)
            text = row[2]
            text_sentences_clean = []
            sentences = re.split(r'(?<=[.!?])\s+', text)  # Split based on punctuation
            for sentence in sentences:
                cleaned_tokens = clean_and_tokenize(sentence)
                if cleaned_tokens:  # Avoid adding empty sentences
                    text_sentences_clean.append(cleaned_tokens)
            sentences_clean.append(text_sentences_clean)

    return sentences_clean

def replace_values_with_one(dictionary, keys_list):
    """
    Replace values with 1 in the dictionary for keys present in the keys_list.
    
    Parameters:
    dictionary (dict): The dictionary to be updated.
    keys_list (list): The list of keys whose values need to be replaced with 1.
    
    Returns:
    dict: The updated dictionary.
    """
    return {k: 1 if k in keys_list else v for k, v in dictionary.items()}

# Step 3: Calculate word frequencies
def calculate_word_frequencies(messages, keys_list):
    """
    Calculates the frequency of each word in the given list of cleaned sentences.
    """
    final_counter = defaultdict(int) # has default value of 0 when key doesn't exist
    for message in messages:
        word_counter = defaultdict(int) # restart for each message to ensure I only count brands once
        for sentence in message:
            for word in sentence:
                word_counter[word] += 1
        # remove multiple mentions of brand
        word_counter = replace_values_with_one(word_counter, keys_list)
        # update final dictionary
        for key in word_counter:
            final_counter[key] += word_counter[key]
    return final_counter

# Step 4: Write word frequencies to CSV
def write_word_frequencies(word_freq, output_file):
    """
    Writes the word frequencies to the specified CSV file.
    """
    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:
        writer = csv.writer(outfile)
        writer.writerow(['Word', 'Frequency'])
        for word, freq in word_freq.items():
            writer.writerow([word, freq])

    print(f"Word frequencies written to {output_file}")

# Main function to run all steps
def main():
    # Step 1: Remove header
    remove_header(input_filename, final_filename)
    
    # Step 2: Extract and clean sentences
    messages_clean = extract_sentences(final_filename)
    
    # Step 3: Calculate word frequencies
    word_freq = calculate_word_frequencies(messages_clean, keys_list)
    
    # Step 4: Write word frequencies to CSV
    write_word_frequencies(word_freq, word_freq_output)

if __name__ == "__main__":
    main()



df = pd.read_csv('word_freq.csv')
df.head(40)


# ALTERNATIVE!
# could use nltk's sentence tokenizer and Count Vectorizer instead?

# Create a tokenizer for splitting on punctuation
tokenizer = RegexpTokenizer(r'\s*[\.\?!]\s*')

text = "Hello world! This is a test. How are you doing today?"

# Tokenize the text into sentences
sentences = tokenizer.tokenize(text)

print(sentences)

# Count Vectorizer




































